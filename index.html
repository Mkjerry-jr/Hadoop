<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Code Sections</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f7f7f7;
            margin: 0;
            padding: 20px;
        }
        .container {
            max-width: 1000px;
            margin: 0 auto;
        }
        h1 {
            text-align: center;
        }
        .section {
            margin-bottom: 20px;
        }
        .header {
            background-color: #007BFF;
            color: white;
            padding: 10px;
            cursor: pointer;
            border-radius: 5px;
            font-weight: bold;
        }
        .content {
            display: none;
            padding: 10px;
            border: 1px solid #ccc;
            border-radius: 5px;
            background-color: white;
            white-space: pre-wrap;
            margin-top: 10px;
        }
        code {
            font-family: "Courier New", monospace;
            font-size: 14px;
            color: #333;
        }
    </style>
</head>
<body>

<div class="container">
    <h1>Expandable Code Sections</h1>

    <!---------------------------------------------------------------------------- Word Count Section -------------------------------------------------------------->
    <div class="section">
        <div class="header" onclick="toggleContent('content1')">Hadoop Word Count Code</div>
        <div class="content" id="content1">
            <code>
import java.io.IOException;
import java.util.StringTokenizer;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {
    public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; {
        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();

        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
            StringTokenizer itr = new StringTokenizer(value.toString());
            while (itr.hasMoreTokens()) {
                word.set(itr.nextToken());
                context.write(word, one);
            }
        }
    }

    public static class IntSumReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {
        private IntWritable result = new IntWritable();

        public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            result.set(sum);
            context.write(key, result);
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "word count");
        job.setJarByClass(WordCount.class);
        job.setMapperClass(TokenizerMapper.class);
        job.setCombinerClass(IntSumReducer.class);
        job.setReducerClass(IntSumReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
            </code>
        </div>
    </div>
    <!------------------------------------------------------------------- WebLog Analysis Section ---------------------------------------------------------->
    <div class="section">
        <div class="header" onclick="toggleContent('content2')">Hadoop WebLog Analysis Code</div>
        <div class="content" id="content2">
            <code>
package Mapreduce;
import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.*;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

public class Mapreduce {
    public static class LogMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {
        private final static IntWritable one = new IntWritable(1);
        private Text ipAddress = new Text();

        public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String[] parts = value.toString().split(" ");
            if (parts.length >= 1) {
                ipAddress.set(parts[0]);
                context.write(ipAddress, one);
            }
        }
    }

    public static class LogReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {
        private IntWritable result = new IntWritable();

        public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            result.set(sum);
            context.write(key, result);
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "log analysis");
        job.setJarByClass(Mapreduce.class);
        job.setMapperClass(LogMapper.class);
        job.setCombinerClass(LogReducer.class);
        job.setReducerClass(LogReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        job.setInputFormatClass(TextInputFormat.class);
        job.setOutputFormatClass(TextOutputFormat.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
		    
Steps: //package name: web , class name: web
1) After Pasting this code and completing build path and Exporting Jar files respectively, do the below steps.
2) Gedit wel.txt    //Write the Input Ip address inside it.
IP Address                    Timestamp of request                 HTTP request            Status Code   Size in Bytes
192.168.0.1                   [24/Apr/2011:04:20:11 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
192.168.0.2                   [24/Apr/2011:04:20:12 -0400]      "GET /dog.jpg HTTP/1.1"    200             14000 
192.168.0.3                   [24/Apr/2011:04:20:13 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
192.168.0.4                   [24/Apr/2011:04:20:14 -0400]      "GET /bird.jpg HTTP/1.1"   200             15000 
192.168.0.5                   [24/Apr/2011:04:20:15 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
192.168.0.6                   [24/Apr/2011:04:20:16 -0400]      "GET /dog.jpg HTTP/1.1"    200             14000 
192.168.0.7                   [24/Apr/2011:04:20:17 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
192.168.0.8                   [24/Apr/2011:04:20:18 -0400]      "GET /fish.jpg HTTP/1.1"   200             13000 
192.168.0.9                   [24/Apr/2011:04:20:19 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
192.168.0.10                  [24/Apr/2011:04:20:20 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
10.0.0.1                      [24/Apr/2011:04:21:01 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
10.0.0.2                      [24/Apr/2011:04:21:02 -0400]      "GET /dog.jpg HTTP/1.1"    200             14000 
10.0.0.3                      [24/Apr/2011:04:21:03 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
10.0.0.4                      [24/Apr/2011:04:21:04 -0400]      "GET /bird.jpg HTTP/1.1"   200             15000 
10.0.0.5                      [24/Apr/2011:04:21:05 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
10.0.0.6                      [24/Apr/2011:04:21:06 -0400]      "GET /dog.jpg HTTP/1.1"    200             14000 
10.0.0.7                      [24/Apr/2011:04:21:07 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
10.0.0.8                      [24/Apr/2011:04:21:08 -0400]      "GET /fish.jpg HTTP/1.1"   200             13000 
10.0.0.9                      [24/Apr/2011:04:21:09 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
10.0.0.10                     [24/Apr/2011:04:21:10 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
96.7.8.17                     [24/Apr/2011:04:22:01 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
96.7.8.18                     [24/Apr/2011:04:22:02 -0400]      "GET /dog.jpg HTTP/1.1"    200             14000 
96.7.8.19                     [24/Apr/2011:04:22:03 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
96.7.8.20                     [24/Apr/2011:04:22:04 -0400]      "GET /bird.jpg HTTP/1.1"   200             15000 
96.7.8.21                     [24/Apr/2011:04:22:05 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
96.7.8.22                     [24/Apr/2011:04:22:06 -0400]      "GET /dog.jpg HTTP/1.1"    200             14000 
96.7.8.23                     [24/Apr/2011:04:22:07 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
96.7.8.24                     [24/Apr/2011:04:22:08 -0400]      "GET /fish.jpg HTTP/1.1"   200             13000 
96.7.8.25                     [24/Apr/2011:04:22:09 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
96.7.8.26                     [24/Apr/2011:04:22:10 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
192.168.1.11                  [24/Apr/2011:04:23:01 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
192.168.1.12                  [24/Apr/2011:04:23:02 -0400]      "GET /dog.jpg HTTP/1.1"    200             14000 
192.168.1.13                  [24/Apr/2011:04:23:03 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
192.168.1.14                  [24/Apr/2011:04:23:04 -0400]      "GET /bird.jpg HTTP/1.1"   200             15000 
192.168.1.15                  [24/Apr/2011:04:23:05 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
192.168.1.16                  [24/Apr/2011:04:23:06 -0400]      "GET /dog.jpg HTTP/1.1"    200             14000 
192.168.1.17                  [24/Apr/2011:04:23:07 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
192.168.1.18                  [24/Apr/2011:04:23:08 -0400]      "GET /fish.jpg HTTP/1.1"   200             13000 
192.168.1.19                  [24/Apr/2011:04:23:09 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
192.168.1.20                  [24/Apr/2011:04:23:10 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
172.16.0.1                     [24/Apr/2011:04:24:01 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
172.16.0.2                     [24/Apr/2011:04:24:02 -0400]      "GET /dog.jpg HTTP/1.1"    200             14000 
172.16.0.3                     [24/Apr/2011:04:24:03 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
172.16.0.4                     [24/Apr/2011:04:24:04 -0400]      "GET /bird.jpg HTTP/1.1"   200             15000 
172.16.0.5                     [24/Apr/2011:04:24:05 -0400]     

3) hadoop fs -put wel.txt wel.txt                        //once type the same command to get file already exists as output
4) hadoop jar web.jar web.web wel.txt dir11              //Try giving different dir numbers for each programs 
5) hadoop fs -cat dir11/part-r-00000    	         //Make sure u give the same dir number wt u gave in above command.	    
            </code>
        </div>
    </div>



    <!-- Temperature Analysis Section -->
    <div class="section">
        <div class="header" onclick="toggleContent('content3')">Hadoop Temperature Analysis Code</div>
        <div class="content" id="content3">
            <code>
package temp;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;
public class temp {
    public static class TemperatureMapper extends Mapper&lt;LongWritable, Text, Text, FloatWritable&gt; {
        private Text year = new Text();
        private FloatWritable temperature = new FloatWritable();
        
        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String line = value.toString();
            String[] fields = line.split(",");
            if (fields.length < 4 || fields[0].trim().equals("Year")) {
                return;
            }
            year.set(fields[0].trim());
            String tempStr = fields[3].trim();
            if (!tempStr.isEmpty()) {
                try {
                    temperature.set(Float.parseFloat(tempStr));
                    context.write(year, temperature);
                } catch (NumberFormatException e) {
                    System.err.println("Error parsing temperature: " + tempStr);
                }
            }
        }
    }
    public static class TemperatureReducer extends Reducer&lt;Text, FloatWritable, Text, FloatWritable&gt; {
        @Override
        protected void reduce(Text key, Iterable&lt;FloatWritable&gt; values, Context context) throws IOException, InterruptedException {
            float maxTemperature = Float.MIN_VALUE;
            for (FloatWritable value : values) {
                maxTemperature = Math.max(maxTemperature, value.get());
            }
            context.write(key, new FloatWritable(maxTemperature));
        }
    }
    public static void main(String[] args) throws Exception {
        if (args.length != 2) {
            System.err.println("Usage: TempAnalysis &lt;input path&gt; &lt;output path&gt;");
            System.exit(-1);
        }
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "Temperature Analysis");
        job.setJarByClass(temp.class);
        job.setMapperClass(TemperatureMapper.class);
        job.setReducerClass(TemperatureReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(FloatWritable.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}

Steps: //package name: temp , class name: temp
1) After Pasting this code and completing build path and Exporting Jar files respectively, do the below steps.
2) Gedit t.txt    //Write the Input temperature inside it.
2024 01 02 03
2024 01 02 03		    
2024 01 02 03
2024 01 02 03
2024 01 02 03	

Real time Data:
Year,Month,Day,Temperature
2020,01,01,27
2021,01,01,25
2022,01,01,20
2023,01,01,23
2020,01,01,15
2021,01,01,22
2022,01,01,25
2023,01,01,19
2020,01,01,16
2021,01,01,22
2022,01,01,20
2023,01,01,29
2020,01,01,28
2021,01,01,23
2022,01,01,24
2023,01,01,19
2020,01,01,16
2021,01,01,24
2022,01,01,26
2023,01,01,22
2020,01,01,18
2021,01,01,27
2022,01,01,29
2023,01,01,21
2020,01,02,18
2021,01,02,24
2022,01,02,15
2023,01,02,29
2020,01,02,16
2021,01,02,22
2022,01,02,27
2023,01,02,26
2020,01,02,15
2021,01,02,29
2022,01,02,25
2023,01,02,28
2020,01,02,29
2021,01,02,24
2022,01,02,27
2023,01,02,20
2020,01,02,19
2021,01,02,21
2022,01,02,28
2023,01,02,22
2020,01,02,17
2021,01,02,18
2022,01,02,27
2023,01,02,23
2020,01,03,16
2021,01,03,28
2022,01,03,19
2023,01,03,15
2020,01,03,29
2021,01,03,23
2022,01,03,25
2023,01,03,21
2020,01,03,28
2021,01,03,29
2022,01,03,27
2023,01,03,18
2020,01,03,22
2021,01,03,20
2022,01,03,29
2023,01,03,27
2020,01,03,15
2021,01,03,21
2022,01,03,22
2023,01,03,17
2020,01,03,29
2021,01,03,22
2022,01,03,19
2023,01,03,28
2020,01,04,24
2021,01,04,27
2022,01,04,25
2023,01,04,16
2020,01,04,28
2021,01,04,19
2022,01,04,24
2023,01,04,29
2020,01,04,23
2021,01,04,20
2022,01,04,22
2023,01,04,25
2020,01,04,27
2021,01,04,15
2022,01,04,29
2023,01,04,16
2020,01,04,18
2021,01,04,19
2022,01,04,24
2023,01,04,28
2020,01,04,27
2021,01,04,22
2022,01,04,21
2023,01,04,17
	    
3) hadoop fs -put t.txt t.txt                              //once type the same command to get file already exists as output
4) hadoop jar temp.jar temp.temp t.txt dir09              //Try giving different dir numbers for each programs
5) hadoop fs -cat t.txt dir09/part-r-00000        		    

            </code>
        </div>
    </div>

    <!-- Stock Analysis Section -->
    <div class="section">
        <div class="header" onclick="toggleContent('content4')">Hadoop Stock Analysis Code</div>
        <div class="content" id="content4">
            <code>
package StockAnalysis1;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import java.io.IOException;

public class StockAnalysis {
    public static class StockMapper extends Mapper&lt;LongWritable, Text, Text, FloatWritable&gt; {
        private Text stockSymbol = new Text();
        private FloatWritable closePrice = new FloatWritable();

        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String line = value.toString().trim();
            if (line.isEmpty()) return;
            String[] fields = line.split(",|\\s+|\\t");
            if (fields.length < 2) return;

            try {
                String stock = fields[1];
                float close = Float.parseFloat(fields[fields.length - 1]);
                stockSymbol.set(stock);
                closePrice.set(close);
                context.write(stockSymbol, closePrice);
            } catch (NumberFormatException e) {}
        }
    }

    public static class StockReducer extends Reducer&lt;Text, FloatWritable, Text, FloatWritable&gt; {
        @Override
        protected void reduce(Text key, Iterable&lt;FloatWritable&gt; values, Context context) throws IOException, InterruptedException {
            float sum = 0;
            int count = 0;
            for (FloatWritable value : values) {
                sum += value.get();
                count++;
            }
            if (count > 0) {
                context.write(key, new FloatWritable(sum / count));
            }
        }
    }

    public static void main(String[] args) throws Exception {
        if (args.length != 2) {
            System.err.println("Usage: StockAnalysis &lt;input path&gt; &lt;output path&gt;");
            System.exit(-1);
        }
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "Stock Analysis");
        job.setJarByClass(StockAnalysis.class);
        job.setMapperClass(StockMapper.class);
        job.setReducerClass(StockReducer.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(FloatWritable.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(FloatWritable.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}

		    
Dummy Data:
2024-01-01     GOOG        1450.00      1465.00     1445.00         1460.00    1200000
2024-01-01     AAPL        300.00       305.00      295.00          304.00     2100000
2024-01-02     GOOG        1460.00      1480.00     1450.00         1470.00    1100000 
		    
Real-Time Data:
Date        Ticker   Open     High     Low      Close    Volume
2024-01-01  MSFT    200.75   205.00   198.00   204.25   1500000
2024-01-01  AMZN    3200.00  3250.00  3150.00  3225.00  1800000
2024-01-01  TSLA    700.12   715.00   695.00   710.00   2000000
2024-01-01  NVDA    500.50   520.00   495.00   515.00   2400000
2024-01-01  META    300.50   310.00   295.00   308.00   2500000
2024-01-01  NFLX    650.75   665.00   640.00   660.00   2200000
2024-01-01  AMD     110.50   115.00   105.00   112.00   2100000
2024-01-01  IBM     135.00   140.00   132.00   138.00   1900000
2024-01-01  ORCL    90.12    92.50    89.00    91.50    2500000
2024-01-01  BABA    150.75   155.00   148.00   153.00   2600000
2024-01-02  MSFT    204.25   208.00   201.00   205.00   1600000
2024-01-02  AMZN    3225.00  3300.00  3200.00  3280.00  1900000
2024-01-02  TSLA    710.00   720.00   705.00   715.00   2100000
2024-01-02  NVDA    515.00   530.00   510.00   525.00   2500000
2024-01-02  META    308.00   315.00   305.00   312.50   2600000
2024-01-02  NFLX    660.00   670.00   650.00   665.00   2300000
2024-01-02  AMD     112.00   116.00   110.00   115.50   2200000
2024-01-02  IBM     138.00   142.00   136.00   141.00   2000000
2024-01-02  ORCL    91.50    94.00    90.00    93.00    2400000
2024-01-02  BABA    153.00   157.00   150.00   156.00   2700000
2024-01-03  MSFT    205.00   209.00   202.00   207.00   1700000
2024-01-03  AMZN    3280.00  3320.00  3250.00  3305.00  2000000
2024-01-03  TSLA    715.00   725.00   710.00   720.00   2200000
2024-01-03  NVDA    525.00   535.00   515.00   530.00   2600000
2024-01-03  META    312.50   318.00   310.00   317.00   2700000
2024-01-03  NFLX    665.00   680.00   660.00   675.00   2400000
2024-01-03  AMD     115.50   118.00   112.00   117.00   2300000
2024-01-03  IBM     141.00   145.00   138.00   144.00   2100000
2024-01-03  ORCL    93.00    95.50    92.00    95.00    2500000
2024-01-03  BABA    156.00   160.00   152.00   158.50   2800000
2024-01-04  MSFT    207.00   210.00   204.00   208.00   1800000
2024-01-04  AMZN    3305.00  3350.00  3280.00  3340.00  2100000
2024-01-04  TSLA    720.00   730.00   715.00   725.00   2300000
2024-01-04  NVDA    530.00   540.00   520.00   535.00   2700000
2024-01-04  META    317.00   322.00   312.00   321.00   2800000
2024-01-04  NFLX    675.00   685.00   670.00   682.00   2500000
2024-01-04  AMD     117.00   120.00   114.00   119.00   2400000
2024-01-04  IBM     144.00   148.00   141.00   146.00   2200000
2024-01-04  ORCL    95.00    98.00    94.00    97.50    2600000
2024-01-04  BABA    158.50   162.00   155.00   161.50   2900000
2024-01-05  MSFT    208.00   212.00   206.00   210.00   1900000
2024-01-05  AMZN    3340.00  3380.00  3300.00  3365.00  2200000
2024-01-05  TSLA    725.00   735.00   720.00   732.00   2400000
2024-01-05  NVDA    535.00   545.00   525.00   540.00   2800000
2024-01-05  META    321.00   326.00   315.00   324.00   2900000
2024-01-05  NFLX    682.00   690.00   675.00   687.00   2600000
2024-01-05  AMD     119.00   122.00   116.00   121.00   2500000
2024-01-05  IBM     146.00   150.00   143.00   148.00   2300000
2024-01-05  ORCL    97.50    100.00   96.00    99.00    2700000
2024-01-05  BABA    161.50   165.00   158.00   164.50   3000000
2024-01-06  MSFT    210.00   214.00   208.00   212.50   2000000
2024-01-06  AMZN    3365.00  3400.00  3320.00  3390.00  2300000
2024-01-06  TSLA    732.00   740.00   725.00   738.00   2500000
2024-01-06  NVDA    540.00   550.00   530.00   545.00   2900000
2024-01-06  META    324.00   330.00   320.00   328.00   3000000
2024-01-06  NFLX    687.00   695.00   680.00   693.00   2700000
2024-01-06  AMD     121.00   124.00   118.00   123.00   2600000
2024-01-06  IBM     148.00   152.00   145.00   150.00   2400000
2024-01-06  ORCL    99.00    102.00   98.00    101.00   2800000
2024-01-06  BABA    164.50   168.00   160.00   167.50   3100000
2024-01-07  MSFT    212.50   216.00   210.00   214.00   2100000
2024-01-07  AMZN    3390.00  3440.00  3350.00  3425.00  2400000
2024-01-07  TSLA    738.00   750.00   730.00   745.00   2600000
2024-01-07  NVDA    545.00   555.00   540.00   550.00   3000000
2024-01-07  META    328.00   335.00   325.00   332.00   3100000
2024-01-07  NFLX    693.00   700.00   688.00   698.00   2800000
2024-01-07  AMD     123.00   126.00   120.00   125.00   2700000
2024-01-07  IBM     150.00   154.00   147.00   152.00   2500000
2024-01-07  ORCL    101.00   105.00   100.00   104.00   2900000
2024-01-07  BABA    167.50   170.00   163.00   169.00   3200000
2024-01-08  MSFT    214.00   218.00   212.00   216.50   2200000
2024-01-08  AMZN    3425.00  3480.00  3380.00  3450.00  2500000
2024-01-08  TSLA    745.00   755.00   740.00   750.00   2700000
2024-01-08  NVDA    550.00   560.00   545.00   555.00   3100000
2024-01-08  META    332.00   338.00   330.00   336.00   3200000
2024-01-08  NFLX    698.00   705.00   692.00   703.00   2900000
2024-01-08  AMD     125.00   128.00   122.00   127.00   2800000
2024-01-08  IBM     152.00   156.00   149.00   154.00   2600000
2024-01-08  ORCL    104.00   108.00   103.00   107.00   3000000
2024-01-08  BABA    169.00   172.00   165.00   171.00   3300000
	    

		    
3) hadoop fs -put s.txt s.txt                                //once type the same command to get file already exists as output
4) hadoop jar stock.jar stock.stock s.txt dir11              //Try giving different dir numbers for each programs 
5) hadoop fs -cat dir11/part-r-00000    	             //Make sure u give the same dir number wt u gave in above command.	    
		    
            </code>
        </div>
    </div>

    <!-- Youtube Section -->
    <div class="section">
        <div class="header" onclick="toggleContent('content5')">Hadoop YouTube Question Code</div>
        <div class="content" id="content5">
            <code>
package reals;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import java.io.IOException;

public class reals {
    public static class StockMapper extends Mapper&lt;LongWritable, Text, Text, FloatWritable&gt; {
        private Text stockSymbol = new Text();
        private FloatWritable closePrice = new FloatWritable();

        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String line = value.toString().trim();
            if (line.isEmpty()) return;
            String[] fields = line.split(",|\\s+|\\t");
            if (fields.length < 2) return;

            try {
                String stock = fields[1];
                float close = Float.parseFloat(fields[fields.length - 1]);
                stockSymbol.set(stock);
                closePrice.set(close);
                context.write(stockSymbol, closePrice);
            } catch (NumberFormatException e) {}
        }
    }

    public static class StockReducer extends Reducer&lt;Text, FloatWritable, Text, FloatWritable&gt; {
        @Override
        protected void reduce(Text key, Iterable&lt;FloatWritable&gt; values, Context context) throws IOException, InterruptedException {
            float sum = 0;
            int count = 0;
            for (FloatWritable value : values) {
                sum += value.get();
                count++;
            }
            if (count > 0) {
                context.write(key, new FloatWritable(sum / count));
            }
        }
    }

    public static void main(String[] args) throws Exception {
        if (args.length != 2) {
            System.err.println("Usage: StockAnalysis &lt;input path&gt; &lt;output path&gt;");
            System.exit(-1);
        }
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "Stock Analysis");
        job.setJarByClass(reals.class);
        job.setMapperClass(StockMapper.class);
        job.setReducerClass(StockReducer.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(FloatWritable.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(FloatWritable.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
		    
		    
Real-Time Data:
Date       Channel_Name     Video_Title          Time    Likes Comments  Views      
2024-10-01 PewDiePie        React_Memes          12:34   15000  2300    450000  
2024-10-02 Dude_Perfect     Trick_Shots          15:22   23000  3400    600000  
2024-10-03 MrBeast         Last_to_Leave       22:11   50000  5400   1200000  
2024-10-04 T-Series        Bollywood_Hits       30:45  120000  5000   1500000  
2024-10-05 Markiplier       Scary_Challenge      18:20   30000  2100    800000  
2024-10-06 James_C         Makeup_Tutorial      25:12   18000  2900    600000  
2024-10-07 Try_Guys        Taste_Test           14:40   25000  1800    450000  
2024-10-08 Fit_Blender      Full_Workout         35:10   20000  1500    300000  
2024-10-09 Jenna_M         DIY_Projects         19:25   16000  2800    700000  
2024-10-10 Vanoss           Gaming_Moments      27:13   21000  3600    900000  
2024-10-11 Rosanna          Baking_Challenge     22:30   17000  2500    550000  
2024-10-12 David_D         Surprise_Pranks      15:00   40000  4000    950000  
2024-10-13 Good_Myth       Food_Hacks           32:55   22000  1900    800000  
2024-10-14 Mr_Mobile       Phone_Reviews       29:15   13000  1700    400000  
2024-10-15 Safiya           Beauty_Hacks         16:45   24000  3300    750000  
2024-10-16 H3_Podcast      Interview            40:05   19000  2400    500000  
2024-10-17 Unbox           Unboxing_Gadgets     14:10   30000  3500    880000  
2024-10-18 Cocomelon       Nursery_Rhymes      45:00   80000 12000   2000000  
2024-10-19 Smosh            Comedy_Skits         28:35   21000  2800    620000  
2024-10-20 Mark_R          Science_Experiments  24:55   28000  3900    850000  
2024-10-21 Dude_P          Bottle_Flip          20:12   17000  2200    520000  
2024-10-22 PewDie          Gaming_Highlights    33:00   26000  3400    780000  
2024-10-23 Jenna_M         Pet_Tips             11:34   12000  1500    300000  
2024-10-24 David_D         Birthday_Surprise    18:22   37000  4100  1000000  
2024-10-25 James_C         Cooking_With_Kids    16:00   23000  2900    650000  
2024-10-26 MrBeast        24_Hour_Challenge    28:40   45000  5000   1300000  
2024-10-27 Dude_Perfect    Epic_Dunks           22:30   21000  3200    710000  
2024-10-28 Try_Guys       DIY_Clothing         19:15   19000  1500    500000  
2024-10-29 Vanoss          Funny_Moments       31:50   28000  3700    820000  
2024-10-30 T-Series        New_Song             25:45   30000  2800    900000  
2024-10-31 Jenna_M        Travel_Vlog          12:45   15000  2100    450000  
2024-11-01 Rosanna         Kitchen_Tips         20:20   17000  1500    520000  
2024-11-02 Good_Myth      Life_Hacks           14:11   22000  3000    600000  
2024-11-03 H3_Podcast     Current_Events       38:30   19000  2200    510000  
2024-11-04 Safiya         DIY_Beauty           27:30   24000  2500    750000  
2024-11-05 Mr_Mobile      Tech_News            15:15   13000  1800    400000  
2024-11-06 Unbox          Latest_Gadgets       17:47   25000  3600    720000  
2024-11-07 Markiplier      Game_Playthrough      30:10   30000  4000    950000  
2024-11-08 PewDiePie      Meme_Review          23:05   28000  4500    860000  
2024-11-09 Jenna_M        Fitness_Challenges    26:15   19000  3200    590000  
2024-11-10 David_D        Reaction_Videos      20:40   37000  4000  1000000  
2024-11-11 MrBeast        Philanthropy         29:55   48000  6200   1300000  
2024-11-12 Cocomelon      Baby_Songs           45:00   90000 13000   2100000  
2024-11-13 Try_Guys       Try_Not_To_Laugh    17:20   16000  2400    540000  
2024-11-14 Vanoss         Epic_Fails          31:30   23000  3100    790000  
2024-11-15 Good_Myth      Snack_Recipes        19:25   22000  1900    800000  
2024-11-16 James_C        Home_Improvement     15:10   17000  2900    600000  
2024-11-17 Safiya        Fashion_Hauls        28:40   24000  3300    850000  
2024-11-18 H3_Podcast     True_Crime          38:00   19000  2400    500000  
2024-11-19 PewDiePie      100_Layer_Challenge  24:15   28000  4500    860000  
2024-11-20 Dude_Perfect    Crazy_Games         20:45   25000  3200    730000  
2024-11-21 Markiplier      Funniest_Moments    22:22   30000  3100    900000  
2024-11-22 Jenna_M        Shopping_Haul        18:30   15000  2700    460000  
2024-11-23 David_D        Reaction_Videos      20:40   37000  4000  1000000  
2024-11-24 Unbox          Gadget_Hacks         15:05   23000  1900    600000  
2024-11-25 Try_Guys       Mythbusters          19:18   16000  2800    650000  
2024-11-26 Cocomelon      Family_Fun           30:00   90000 12000   2000000  
2024-11-27 MrBeast        Survival_Challenge   27:10   48000  5000   1400000  
2024-11-28 Vanoss         Game_Night           29:50   21000  3200    780000  
2024-11-29 Safiya         Holiday_Treats       20:10   24000  2700    700000  
2024-11-30 Good_Myth      Pet_Hacks            14:50   22000  1900    590000  
2024-12-01 H3_Podcast     Review_Series        38:00   19000  2100    520000  
2024-12-02 James_C        Skill_Building       15:55   17000  2300    640000  
2024-12-03 Try_Guys       Taste_Test           20:45   25000  2200    670000  
2024-12-04 PewDiePie      Game_Room            22:12   28000  3200    830000  
2024-12-05 Jenna_M        Home_Decor           26:35   15000  1800    490000  
2024-12-06 Markiplier      Scary_Stories        18:20   30000  3500    720000  
2024-12-07 David_D        Creative_Ideas       20:15   37000  4000  1100000  
2024-12-08 Dude_Perfect    Skills_Challenge     15:30   23000  2600    600000  
2024-12-09 MrBeast        Charity_Event        40:12   48000  5400   1200000  
2024-12-10 T-Series        Latest_Single        30:45  120000  5000   1500000
2024-10-11 Rosanna          Baking_Challenge     22:30   17000  2500    550000  
2024-10-12 David_D         Surprise_Pranks      15:00   40000  4000    950000  
2024-10-13 Good_Myth       Food_Hacks           32:55   22000  1900    800000  
2024-10-14 Mr_Mobile       Phone_Reviews       29:15   13000  1700    400000  
2024-10-15 Safiya           Beauty_Hacks         16:45   24000  3300    750000  
2024-10-16 H3_Podcast      Interview            40:05   19000  2400    500000  
2024-10-17 Unbox           Unboxing_Gadgets     14:10   30000  3500    880000  
2024-10-18 Cocomelon       Nursery_Rhymes      45:00   80000 12000   2000000  
2024-10-19 Smosh            Comedy_Skits         28:35   21000  2800    620000  
2024-10-20 Mark_R          Science_Experiments  24:55   28000  3900    850000  
2024-10-21 Dude_P          Bottle_Flip          20:12   17000  2200    520000  
2024-10-22 PewDie          Gaming_Highlights    33:00   26000  3400    780000  
2024-10-23 Jenna_M         Pet_Tips             11:34   12000  1500    300000  
2024-10-24 David_D         Birthday_Surprise    18:22   37000  4100  1000000  
2024-10-25 James_C         Cooking_With_Kids    16:00   23000  2900    650000  
2024-10-26 MrBeast        24_Hour_Challenge    28:40   45000  5000   1300000  
2024-10-27 Dude_Perfect    Epic_Dunks           22:30   21000  3200    710000  
2024-10-28 Try_Guys       DIY_Clothing         19:15   19000  1500    500000  
2024-10-29 Vanoss          Funny_Moments       31:50   28000  3700    820000  
2024-10-30 T-Series        New_Song             25:45   30000  2800    900000  
2024-10-31 Jenna_M        Travel_Vlog          12:45   15000  2100    450000 		    


3)hadoop fs -put yt.txt yt.txt                                     //once type the same command to get file already exists as output
4)hadoop jar reals.jar reals.reals yt.txt dir11                   //Try giving different dir numbers for each programs 
5)hadoop fs -cat dir11/part-r-00000    	                         //Make sure u give the same dir number wt u gave in above command.	    	    
            </code>
        </div>
    </div>

<!-------------------------------------------------------HIVE-------------------------------------------------------------------------->
<div class="section">
        <div class="header" onclick="toggleContent('content7')">Introduction to HIVE</div>
        <div class="content" id="content7">
            <code>
1)open terminal and type hive:
		    
hive
___________________________________________________________________________________________________________
		    
2)To display the existing databases:
		    
show databases;
___________________________________________________________________________________________________________
		    
3)To create a new database: (Database name is "demo")
		    
create database demo; create database student_163;
___________________________________________________________________________________________________________

4)To select a required database: (Selected database is "demo")
		    
use demo;
___________________________________________________________________________________________________________
		    
5)To create a table in the selected database: (Table name is "student")

create table student(name string,rollno string,age int)row format delimited fields terminated by ',';
___________________________________________________________________________________________________________

6)To insert data into table "student" from a file "Student1" that is stored in local file system:

load data local inpath '/home/training/Desktop/Student1' into table student;
select * from student;
converted to mapreduce jobs 
___________________________________________________________________________________________________________	

7)To insert some more data into table "student" from a file "Student2" that is stored in local file system:
		    
load data local inpath '/home/training/Desktop/Student2' into table student;
___________________________________________________________________________________________________________

8)To insert some more data into table "student" from a file "Student3" that is stored in local file system:
(Student 3 has some field types different from what the schema is defined)
		    
load data local inpath '/home/training/Desktop/Student3' into table student;
___________________________________________________________________________________________________________

9)To display metadata about the table:
		    
describe student;
___________________________________________________________________________________________________________

10)To display detailed metadata about the table:
		    
describe extended student;
___________________________________________________________________________________________________________

11)To insert data into table "student" from a file "Student4" that is stored in hdfs:
   First load the data from local file sytem into hdfs. This can be done using the following command:
    
   $ cd desktop
   $ hadoop dfs -put stu4 /student4.txt
   Then load the data from hdfs into hive table: 
   load data inpath '/student4.txt' into table student;
		    
Note: once loaded the file "Student4" will be moved from its path into given hive table "student"
___________________________________________________________________________________________________________

12)To display the contents of table "student":
		    
select count(*) from student;

___________________________________________________________________________________________________________

13)To display the name "Akash" in uppercase:
		    
select upper(name)
from student
where name='Akash';
___________________________________________________________________________________________________________
Execute some more select with aggregate functions	
___________________________________________________________________________________________________________

14)To create external table "studextern" in the path "/hivedata":
		    
     create external table studextern(
     name string,                     
     rollno string,                   
     age int                          
     )row format delimited            
     fields terminated by ','         
     location '/hivedata';
___________________________________________________________________________________________________________

15)To insert data into external table "studextern":
		    
    hadoop dfs -put Student1 /hivedata
    Note: Just load the file into "/hivedata". All the contents of the file will be inserted into the
    table "studextern"

___________________________________________________________________________________________________________

16)To display the contents of table "studextern"
		    
     select * from studextern;

Note: Internal table will be in "/user/hive/warehouse"
Note: External table will be in the path where the data is stored.
___________________________________________________________________________________________________________

17) To remove internal table "student":
		    
    drop table student;
___________________________________________________________________________________________________________

18) To remove external table "studextern":
		    
    drop table studextern;
___________________________________________________________________________________________________________		    

19) To remove database:
		    
    drop database demo;
		    
Note: When external table is dropped its schema is dropped but data still exists. Can be checked in browser.
Note: When internal table is dropped its schema and also data is deleted.
___________________________________________________________________________________________________________		    


            </code>
        </div>
    </div>
	
<!-- User Guide -->
    <div class="section">
        <div class="header" onclick="toggleContent('content6')">JAR FILES GUIDE</div>
        <div class="content" id="content6">
            <code>  
		    
AFTER CODE DO THIS:
Build Path-> Add External Archives-> File System-> Usr-> Lib-> hadoop.common.jar
Build Path-> Add External Archives-> File System-> Usr-> Lib-> hadoop-0.20-mapreduce-> hadoop-core-2.6.0-mr1-cdh5.13.0.jar
Build Path-> Add External Archives-> File System-> Usr-> Lib-> hadoop-> hadoop-auth.jar	

This will make to remove all the red lines in the copied code, if not, copy paste the code again and check the class name and package name properly.

Export the class File:
Export-> Jar File-> "Select the path:" /home/cloudera/"your_jar_file_name".jar   ----> Finish
		     
            </code>
        </div>
    </div>
</div> 	
</div>

<script>
function toggleContent(id) {
    var content = document.getElementById(id);
    if (content.style.display === "block") {
        content.style.display = "none";
    } else {
        content.style.display = "block";
    }
}
</script>

</body>
</html>

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Code Sections</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f7f7f7;
            margin: 0;
            padding: 20px;
        }
        .container {
            max-width: 1000px;
            margin: 0 auto;
        }
        h1 {
            text-align: center;
        }
        .section {
            margin-bottom: 20px;
        }
        .header {
            background-color: #007BFF;
            color: white;
            padding: 10px;
            cursor: pointer;
            border-radius: 5px;
            font-weight: bold;
        }
        .content {
            display: none;
            padding: 10px;
            border: 1px solid #ccc;
            border-radius: 5px;
            background-color: white;
            white-space: pre-wrap;
            margin-top: 10px;
        }
        code {
            font-family: "Courier New", monospace;
            font-size: 14px;
            color: #333;
        }
    </style>
</head>
<body>

<div class="container">
    <h1>Expandable Code Sections</h1>

    <!---------------------------------------------------------------------------- Word Count Section -------------------------------------------------------------->
    <div class="section">
        <div class="header" onclick="toggleContent('content1')">Hadoop Word Count Code</div>
        <div class="content" id="content1">
            <code>
import java.io.IOException;
import java.util.StringTokenizer;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {
    public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; {
        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();

        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
            StringTokenizer itr = new StringTokenizer(value.toString());
            while (itr.hasMoreTokens()) {
                word.set(itr.nextToken());
                context.write(word, one);
            }
        }
    }

    public static class IntSumReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {
        private IntWritable result = new IntWritable();

        public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            result.set(sum);
            context.write(key, result);
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "word count");
        job.setJarByClass(WordCount.class);
        job.setMapperClass(TokenizerMapper.class);
        job.setCombinerClass(IntSumReducer.class);
        job.setReducerClass(IntSumReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
            </code>
        </div>
    </div>
    <!------------------------------------------------------------------- WebLog Analysis Section ---------------------------------------------------------->
    <div class="section">
        <div class="header" onclick="toggleContent('content2')">Hadoop WebLog Analysis Code</div>
        <div class="content" id="content2">
            <code>
package Mapreduce;
import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.*;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

public class Mapreduce {
    public static class LogMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {
        private final static IntWritable one = new IntWritable(1);
        private Text ipAddress = new Text();

        public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String[] parts = value.toString().split(" ");
            if (parts.length >= 1) {
                ipAddress.set(parts[0]);
                context.write(ipAddress, one);
            }
        }
    }

    public static class LogReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {
        private IntWritable result = new IntWritable();

        public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            result.set(sum);
            context.write(key, result);
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "log analysis");
        job.setJarByClass(Mapreduce.class);
        job.setMapperClass(LogMapper.class);
        job.setCombinerClass(LogReducer.class);
        job.setReducerClass(LogReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        job.setInputFormatClass(TextInputFormat.class);
        job.setOutputFormatClass(TextOutputFormat.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
		    
Steps: //package name: web , class name: web
1) After Pasting this code and completing build path and Exporting Jar files respectively, do the below steps.
2) Gedit wel.txt    //Write the Input Ip address inside it.
IP Address                    Timestamp of request                 HTTP request            Status Code   Size in Bytes
192.168.0.1                   [24/Apr/2011:04:20:11 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
192.168.0.2                   [24/Apr/2011:04:20:12 -0400]      "GET /dog.jpg HTTP/1.1"    200             14000 
192.168.0.3                   [24/Apr/2011:04:20:13 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
192.168.0.4                   [24/Apr/2011:04:20:14 -0400]      "GET /bird.jpg HTTP/1.1"   200             15000 
192.168.0.5                   [24/Apr/2011:04:20:15 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
192.168.0.6                   [24/Apr/2011:04:20:16 -0400]      "GET /dog.jpg HTTP/1.1"    200             14000 
192.168.0.7                   [24/Apr/2011:04:20:17 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
192.168.0.8                   [24/Apr/2011:04:20:18 -0400]      "GET /fish.jpg HTTP/1.1"   200             13000 
192.168.0.9                   [24/Apr/2011:04:20:19 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
192.168.0.10                  [24/Apr/2011:04:20:20 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
10.0.0.1                      [24/Apr/2011:04:21:01 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
10.0.0.2                      [24/Apr/2011:04:21:02 -0400]      "GET /dog.jpg HTTP/1.1"    200             14000 
10.0.0.3                      [24/Apr/2011:04:21:03 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
10.0.0.4                      [24/Apr/2011:04:21:04 -0400]      "GET /bird.jpg HTTP/1.1"   200             15000 
10.0.0.5                      [24/Apr/2011:04:21:05 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
10.0.0.6                      [24/Apr/2011:04:21:06 -0400]      "GET /dog.jpg HTTP/1.1"    200             14000 
10.0.0.7                      [24/Apr/2011:04:21:07 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
10.0.0.8                      [24/Apr/2011:04:21:08 -0400]      "GET /fish.jpg HTTP/1.1"   200             13000 
10.0.0.9                      [24/Apr/2011:04:21:09 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
10.0.0.10                     [24/Apr/2011:04:21:10 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
96.7.8.17                     [24/Apr/2011:04:22:01 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
96.7.8.18                     [24/Apr/2011:04:22:02 -0400]      "GET /dog.jpg HTTP/1.1"    200             14000 
96.7.8.19                     [24/Apr/2011:04:22:03 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
96.7.8.20                     [24/Apr/2011:04:22:04 -0400]      "GET /bird.jpg HTTP/1.1"   200             15000 
96.7.8.21                     [24/Apr/2011:04:22:05 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
96.7.8.22                     [24/Apr/2011:04:22:06 -0400]      "GET /dog.jpg HTTP/1.1"    200             14000 
96.7.8.23                     [24/Apr/2011:04:22:07 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
96.7.8.24                     [24/Apr/2011:04:22:08 -0400]      "GET /fish.jpg HTTP/1.1"   200             13000 
96.7.8.25                     [24/Apr/2011:04:22:09 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
96.7.8.26                     [24/Apr/2011:04:22:10 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
192.168.1.11                  [24/Apr/2011:04:23:01 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
192.168.1.12                  [24/Apr/2011:04:23:02 -0400]      "GET /dog.jpg HTTP/1.1"    200             14000 
192.168.1.13                  [24/Apr/2011:04:23:03 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
192.168.1.14                  [24/Apr/2011:04:23:04 -0400]      "GET /bird.jpg HTTP/1.1"   200             15000 
192.168.1.15                  [24/Apr/2011:04:23:05 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
192.168.1.16                  [24/Apr/2011:04:23:06 -0400]      "GET /dog.jpg HTTP/1.1"    200             14000 
192.168.1.17                  [24/Apr/2011:04:23:07 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
192.168.1.18                  [24/Apr/2011:04:23:08 -0400]      "GET /fish.jpg HTTP/1.1"   200             13000 
192.168.1.19                  [24/Apr/2011:04:23:09 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
192.168.1.20                  [24/Apr/2011:04:23:10 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
172.16.0.1                     [24/Apr/2011:04:24:01 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
172.16.0.2                     [24/Apr/2011:04:24:02 -0400]      "GET /dog.jpg HTTP/1.1"    200             14000 
172.16.0.3                     [24/Apr/2011:04:24:03 -0400]      "GET /cat.jpg HTTP/1.1"    200             12433 
172.16.0.4                     [24/Apr/2011:04:24:04 -0400]      "GET /bird.jpg HTTP/1.1"   200             15000 
172.16.0.5                     [24/Apr/2011:04:24:05 -0400]     

3)hadoop fs -put wel.txt wel.txt                                //once type the same command to get file already exists as output
4)hadoop jar web.jar web.web wel.txt dir11                     //Try giving different dir numbers for each programs 
5)hadoop fs -cat dir11/part-00000    	                      //Make sure u give the same dir number wt u gave in above command.	    
            </code>
        </div>
    </div>



    <!-- Temperature Analysis Section -->
    <div class="section">
        <div class="header" onclick="toggleContent('content3')">Hadoop Temperature Analysis Code</div>
        <div class="content" id="content3">
            <code>
package temp;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;
public class temp {
    public static class TemperatureMapper extends Mapper&lt;LongWritable, Text, Text, FloatWritable&gt; {
        private Text year = new Text();
        private FloatWritable temperature = new FloatWritable();
        
        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String line = value.toString();
            String[] fields = line.split(",");
            if (fields.length < 4 || fields[0].trim().equals("Year")) {
                return;
            }
            year.set(fields[0].trim());
            String tempStr = fields[3].trim();
            if (!tempStr.isEmpty()) {
                try {
                    temperature.set(Float.parseFloat(tempStr));
                    context.write(year, temperature);
                } catch (NumberFormatException e) {
                    System.err.println("Error parsing temperature: " + tempStr);
                }
            }
        }
    }
    public static class TemperatureReducer extends Reducer&lt;Text, FloatWritable, Text, FloatWritable&gt; {
        @Override
        protected void reduce(Text key, Iterable&lt;FloatWritable&gt; values, Context context) throws IOException, InterruptedException {
            float maxTemperature = Float.MIN_VALUE;
            for (FloatWritable value : values) {
                maxTemperature = Math.max(maxTemperature, value.get());
            }
            context.write(key, new FloatWritable(maxTemperature));
        }
    }
    public static void main(String[] args) throws Exception {
        if (args.length != 2) {
            System.err.println("Usage: TempAnalysis &lt;input path&gt; &lt;output path&gt;");
            System.exit(-1);
        }
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "Temperature Analysis");
        job.setJarByClass(temp.class);
        job.setMapperClass(TemperatureMapper.class);
        job.setReducerClass(TemperatureReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(FloatWritable.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}

Steps: //package name: temp , class name: temp
1) After Pasting this code and completing build path and Exporting Jar files respectively, do the below steps.
2) Gedit t.txt    //Write the Input temperature inside it.
2024 01 02 03
2024 01 02 03		    
2024 01 02 03
2024 01 02 03
2024 01 02 03	

Real time Data:
Year, Month, Day, Hour, Temperature
2020, 01, 01, 00, 27.45
2021, 01, 01, 01, 25.16
2022, 01, 01, 02, 20.03
2023, 01, 01, 03, 23.87
2020, 01, 01, 04, 15.47
2021, 01, 01, 05, 22.57
2022, 01, 01, 06, 25.86
2023, 01, 01, 07, 19.27
2020, 01, 01, 08, 16.54
2021, 01, 01, 09, 22.66
2022, 01, 01, 10, 20.33
2023, 01, 01, 11, 29.75
2020, 01, 01, 12, 28.14
2021, 01, 01, 13, 23.69
2022, 01, 01, 14, 24.84
2023, 01, 01, 15, 19.31
2020, 01, 01, 16, 16.85
2021, 01, 01, 17, 24.92
2022, 01, 01, 18, 26.68
2023, 01, 01, 19, 22.12
2020, 01, 01, 20, 18.38
2021, 01, 01, 21, 27.01
2022, 01, 01, 22, 29.62
2023, 01, 01, 23, 21.17
2020, 01, 02, 00, 18.94
2021, 01, 02, 01, 24.02
2022, 01, 02, 02, 15.19
2023, 01, 02, 03, 29.35
2020, 01, 02, 04, 16.77
2021, 01, 02, 05, 22.79
2022, 01, 02, 06, 27.23
2023, 01, 02, 07, 26.46
2020, 01, 02, 08, 15.78
2021, 01, 02, 09, 29.90
2022, 01, 02, 10, 25.67
2023, 01, 02, 11, 28.08
2020, 01, 02, 12, 29.88
2021, 01, 02, 13, 24.13
2022, 01, 02, 14, 27.65
2023, 01, 02, 15, 20.81
2020, 01, 02, 16, 19.14
2021, 01, 02, 17, 21.95
2022, 01, 02, 18, 28.58
2023, 01, 02, 19, 22.29
2020, 01, 02, 20, 17.76
2021, 01, 02, 21, 18.36
2022, 01, 02, 22, 27.47
2023, 01, 02, 23, 23.45
2020, 01, 03, 00, 16.23
2021, 01, 03, 01, 28.49
2022, 01, 03, 02, 19.94
2023, 01, 03, 03, 15.89
2020, 01, 03, 04, 29.01
2021, 01, 03, 05, 23.57
2022, 01, 03, 06, 25.40
2023, 01, 03, 07, 21.87
2020, 01, 03, 08, 28.73
2021, 01, 03, 09, 29.90
2022, 01, 03, 10, 27.84
2023, 01, 03, 11, 18.62
2020, 01, 03, 12, 22.75
2021, 01, 03, 13, 20.33
2022, 01, 03, 14, 29.14
2023, 01, 03, 15, 27.09
2020, 01, 03, 16, 15.56
2021, 01, 03, 17, 21.38
2022, 01, 03, 18, 22.81
2023, 01, 03, 19, 17.12
2020, 01, 03, 20, 29.62
2021, 01, 03, 21, 22.84
2022, 01, 03, 22, 19.31
2023, 01, 03, 23, 28.59
2020, 01, 04, 00, 24.02
2021, 01, 04, 01, 27.03
2022, 01, 04, 02, 25.55
2023, 01, 04, 03, 16.66
2020, 01, 04, 04, 28.36
2021, 01, 04, 05, 19.85
2022, 01, 04, 06, 24.23
2023, 01, 04, 07, 29.44
2020, 01, 04, 08, 23.78
2021, 01, 04, 09, 20.67
2022, 01, 04, 10, 22.81
2023, 01, 04, 11, 25.39
2020, 01, 04, 12, 27.58
2021, 01, 04, 13, 15.90
2022, 01, 04, 14, 29.81
2023, 01, 04, 15, 16.59
2020, 01, 04, 16, 18.77
2021, 01, 04, 17, 19.54
2022, 01, 04, 18, 24.92
2023, 01, 04, 19, 28.01
2020, 01, 04, 20, 27.35
2021, 01, 04, 21, 22.58
2022, 01, 04, 22, 21.63
2023, 01, 04, 23, 17.87
	    
3)hadoop fs -put t.txt t.txt                                    //once type the same command to get file already exists as output
4)hadoop jar temp.jar temp.temp t.txt dir09                    //Try giving different dir numbers for each programs
5)hadoop fs -cat t.txt dir09/part-00000        		    

            </code>
        </div>
    </div>

    <!-- Stock Analysis Section -->
    <div class="section">
        <div class="header" onclick="toggleContent('content4')">Hadoop Stock Analysis Code</div>
        <div class="content" id="content4">
            <code>
package StockAnalysis1;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import java.io.IOException;

public class StockAnalysis {
    public static class StockMapper extends Mapper&lt;LongWritable, Text, Text, FloatWritable&gt; {
        private Text stockSymbol = new Text();
        private FloatWritable closePrice = new FloatWritable();

        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String line = value.toString().trim();
            if (line.isEmpty()) return;
            String[] fields = line.split(",|\\s+|\\t");
            if (fields.length < 2) return;

            try {
                String stock = fields[1];
                float close = Float.parseFloat(fields[fields.length - 1]);
                stockSymbol.set(stock);
                closePrice.set(close);
                context.write(stockSymbol, closePrice);
            } catch (NumberFormatException e) {}
        }
    }

    public static class StockReducer extends Reducer&lt;Text, FloatWritable, Text, FloatWritable&gt; {
        @Override
        protected void reduce(Text key, Iterable&lt;FloatWritable&gt; values, Context context) throws IOException, InterruptedException {
            float sum = 0;
            int count = 0;
            for (FloatWritable value : values) {
                sum += value.get();
                count++;
            }
            if (count > 0) {
                context.write(key, new FloatWritable(sum / count));
            }
        }
    }

    public static void main(String[] args) throws Exception {
        if (args.length != 2) {
            System.err.println("Usage: StockAnalysis &lt;input path&gt; &lt;output path&gt;");
            System.exit(-1);
        }
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "Stock Analysis");
        job.setJarByClass(StockAnalysis.class);
        job.setMapperClass(StockMapper.class);
        job.setReducerClass(StockReducer.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(FloatWritable.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(FloatWritable.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}

		    
Dummy Data:
2024-01-01     GOOG        1450.00      1465.00     1445.00         1460.00    1200000
2024-01-01     AAPL        300.00       305.00      295.00          304.00     2100000
2024-01-02     GOOG        1460.00      1480.00     1450.00         1470.00    1100000 
		    
Real-Time Data:
Date        Ticker   Open     High     Low      Close    Volume
2024-01-01  MSFT    200.75   205.00   198.00   204.25   1500000
2024-01-01  AMZN    3200.00  3250.00  3150.00  3225.00  1800000
2024-01-01  TSLA    700.12   715.00   695.00   710.00   2000000
2024-01-01  NVDA    500.50   520.00   495.00   515.00   2400000
2024-01-01  META    300.50   310.00   295.00   308.00   2500000
2024-01-01  NFLX    650.75   665.00   640.00   660.00   2200000
2024-01-01  AMD     110.50   115.00   105.00   112.00   2100000
2024-01-01  IBM     135.00   140.00   132.00   138.00   1900000
2024-01-01  ORCL    90.12    92.50    89.00    91.50    2500000
2024-01-01  BABA    150.75   155.00   148.00   153.00   2600000
2024-01-02  MSFT    204.25   208.00   201.00   205.00   1600000
2024-01-02  AMZN    3225.00  3300.00  3200.00  3280.00  1900000
2024-01-02  TSLA    710.00   720.00   705.00   715.00   2100000
2024-01-02  NVDA    515.00   530.00   510.00   525.00   2500000
2024-01-02  META    308.00   315.00   305.00   312.50   2600000
2024-01-02  NFLX    660.00   670.00   650.00   665.00   2300000
2024-01-02  AMD     112.00   116.00   110.00   115.50   2200000
2024-01-02  IBM     138.00   142.00   136.00   141.00   2000000
2024-01-02  ORCL    91.50    94.00    90.00    93.00    2400000
2024-01-02  BABA    153.00   157.00   150.00   156.00   2700000
2024-01-03  MSFT    205.00   209.00   202.00   207.00   1700000
2024-01-03  AMZN    3280.00  3320.00  3250.00  3305.00  2000000
2024-01-03  TSLA    715.00   725.00   710.00   720.00   2200000
2024-01-03  NVDA    525.00   535.00   515.00   530.00   2600000
2024-01-03  META    312.50   318.00   310.00   317.00   2700000
2024-01-03  NFLX    665.00   680.00   660.00   675.00   2400000
2024-01-03  AMD     115.50   118.00   112.00   117.00   2300000
2024-01-03  IBM     141.00   145.00   138.00   144.00   2100000
2024-01-03  ORCL    93.00    95.50    92.00    95.00    2500000
2024-01-03  BABA    156.00   160.00   152.00   158.50   2800000
2024-01-04  MSFT    207.00   210.00   204.00   208.00   1800000
2024-01-04  AMZN    3305.00  3350.00  3280.00  3340.00  2100000
2024-01-04  TSLA    720.00   730.00   715.00   725.00   2300000
2024-01-04  NVDA    530.00   540.00   520.00   535.00   2700000
2024-01-04  META    317.00   322.00   312.00   321.00   2800000
2024-01-04  NFLX    675.00   685.00   670.00   682.00   2500000
2024-01-04  AMD     117.00   120.00   114.00   119.00   2400000
2024-01-04  IBM     144.00   148.00   141.00   146.00   2200000
2024-01-04  ORCL    95.00    98.00    94.00    97.50    2600000
2024-01-04  BABA    158.50   162.00   155.00   161.50   2900000
2024-01-05  MSFT    208.00   212.00   206.00   210.00   1900000
2024-01-05  AMZN    3340.00  3380.00  3300.00  3365.00  2200000
2024-01-05  TSLA    725.00   735.00   720.00   732.00   2400000
2024-01-05  NVDA    535.00   545.00   525.00   540.00   2800000
2024-01-05  META    321.00   326.00   315.00   324.00   2900000
2024-01-05  NFLX    682.00   690.00   675.00   687.00   2600000
2024-01-05  AMD     119.00   122.00   116.00   121.00   2500000
2024-01-05  IBM     146.00   150.00   143.00   148.00   2300000
2024-01-05  ORCL    97.50    100.00   96.00    99.00    2700000
2024-01-05  BABA    161.50   165.00   158.00   164.50   3000000
2024-01-06  MSFT    210.00   214.00   208.00   212.50   2000000
2024-01-06  AMZN    3365.00  3400.00  3320.00  3390.00  2300000
2024-01-06  TSLA    732.00   740.00   725.00   738.00   2500000
2024-01-06  NVDA    540.00   550.00   530.00   545.00   2900000
2024-01-06  META    324.00   330.00   320.00   328.00   3000000
2024-01-06  NFLX    687.00   695.00   680.00   693.00   2700000
2024-01-06  AMD     121.00   124.00   118.00   123.00   2600000
2024-01-06  IBM     148.00   152.00   145.00   150.00   2400000
2024-01-06  ORCL    99.00    102.00   98.00    101.00   2800000
2024-01-06  BABA    164.50   168.00   160.00   167.50   3100000
2024-01-07  MSFT    212.50   216.00   210.00   214.00   2100000
2024-01-07  AMZN    3390.00  3440.00  3350.00  3425.00  2400000
2024-01-07  TSLA    738.00   750.00   730.00   745.00   2600000
2024-01-07  NVDA    545.00   555.00   540.00   550.00   3000000
2024-01-07  META    328.00   335.00   325.00   332.00   3100000
2024-01-07  NFLX    693.00   700.00   688.00   698.00   2800000
2024-01-07  AMD     123.00   126.00   120.00   125.00   2700000
2024-01-07  IBM     150.00   154.00   147.00   152.00   2500000
2024-01-07  ORCL    101.00   105.00   100.00   104.00   2900000
2024-01-07  BABA    167.50   170.00   163.00   169.00   3200000
2024-01-08  MSFT    214.00   218.00   212.00   216.50   2200000
2024-01-08  AMZN    3425.00  3480.00  3380.00  3450.00  2500000
2024-01-08  TSLA    745.00   755.00   740.00   750.00   2700000
2024-01-08  NVDA    550.00   560.00   545.00   555.00   3100000
2024-01-08  META    332.00   338.00   330.00   336.00   3200000
2024-01-08  NFLX    698.00   705.00   692.00   703.00   2900000
2024-01-08  AMD     125.00   128.00   122.00   127.00   2800000
2024-01-08  IBM     152.00   156.00   149.00   154.00   2600000
2024-01-08  ORCL    104.00   108.00   103.00   107.00   3000000
2024-01-08  BABA    169.00   172.00   165.00   171.00   3300000
	    

		    
3)hadoop fs -put s.txt s.txt                                     //once type the same command to get file already exists as output
4)hadoop jar stock.jar stock.stock s.txt dir11                   //Try giving different dir numbers for each programs 
5)hadoop fs -cat dir11/part-00000    	                         //Make sure u give the same dir number wt u gave in above command.	    
		    
            </code>
        </div>
    </div>

    <!-- Youtube Section -->
    <div class="section">
        <div class="header" onclick="toggleContent('content5')">Hadoop YouTube Question Code</div>
        <div class="content" id="content5">
            <code>
package reals;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import java.io.IOException;

public class reals {
    public static class StockMapper extends Mapper&lt;LongWritable, Text, Text, FloatWritable&gt; {
        private Text stockSymbol = new Text();
        private FloatWritable closePrice = new FloatWritable();

        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String line = value.toString().trim();
            if (line.isEmpty()) return;
            String[] fields = line.split(",|\\s+|\\t");
            if (fields.length < 2) return;

            try {
                String stock = fields[1];
                float close = Float.parseFloat(fields[fields.length - 1]);
                stockSymbol.set(stock);
                closePrice.set(close);
                context.write(stockSymbol, closePrice);
            } catch (NumberFormatException e) {}
        }
    }

    public static class StockReducer extends Reducer&lt;Text, FloatWritable, Text, FloatWritable&gt; {
        @Override
        protected void reduce(Text key, Iterable&lt;FloatWritable&gt; values, Context context) throws IOException, InterruptedException {
            float sum = 0;
            int count = 0;
            for (FloatWritable value : values) {
                sum += value.get();
                count++;
            }
            if (count > 0) {
                context.write(key, new FloatWritable(sum / count));
            }
        }
    }

    public static void main(String[] args) throws Exception {
        if (args.length != 2) {
            System.err.println("Usage: StockAnalysis &lt;input path&gt; &lt;output path&gt;");
            System.exit(-1);
        }
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "Stock Analysis");
        job.setJarByClass(reals.class);
        job.setMapperClass(StockMapper.class);
        job.setReducerClass(StockReducer.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(FloatWritable.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(FloatWritable.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
		    
		    
Real-Time Data:
Date       Channel_Name         Video_Title          Time Likes Comments Views      
2024-10-03 T-Series             Live_Piano           35:23  9700 1243 162354
2024-10-05 Yoga_With_Adriene    Yoga_Athletes        14:35  4200  302 104321
2024-10-07 TechBurner           Puzzle_Tips          18:22  6700  724 139472
2024-10-05 GeekyRanjit          Electric_Scooter     13:10  5700  398 127593
2024-10-06 BB_Ki_Vines          Indie_Artists        58:17  7600 1102 148361
2024-10-03 Fit_Tuber            Pilates_Beginners    12:03  3900  312  97384
2024-10-06 Slayy_Point          Smartwatch_Review    14:56  4600  371 111243
2024-10-06 Coke_Studio_India    Jazz_Live            35:10  9400 1263 174982
2024-10-05 Flying_Beast         Full_Stretch         18:45  3500  267 100294
2024-10-06 Mortal               Adventure_Tips       22:03  7500  684 135872
2024-10-07 Emiway_Bantai        EDM_Tracks           41:48  9100 1352 169543
2024-10-07 Yoga_With_Adriene    Mindfulness          16:34  3100  235 103592
2024-10-06 TechBurner           Co-op_Review         17:34  6700  598 138472
2024-10-07 T-Series             Indie_Tracks         36:12 10600 1420 187431
2024-10-05 GeekyRanjit          Headphones_Review    11:47  5500  387 122348
2024-10-07 Emiway_Bantai        Acoustic_Performance 47:21  7200 1092 143872
2024-10-04 BB_Ki_Vines          Soul_Collection      52:10  7900 1204 154923
2024-10-05 Fit_Tuber            Bodyweight_Workout   15:12  4500  389 112834
2024-10-03 TechBurner           Strategy_Guide       21:03  7100  653 125981
2024-10-07 Technical_Guruji     Smart_Devices        13:45  6100  453 129432
2024-10-06 BB_Ki_Vines          DJ_Set               23:12  9600 1487 178243
2024-10-05 Emiway_Bantai        Rock_Anthems         54:57  8200 1278 167432
2024-10-07 Flying_Beast         Strength_Home        16:48  4200  365 109812
2024-10-05 Total_Gaming         MMORPG_Tutorial      19:27  7200  698 138765
2024-10-07 Technical_Guruji     4K_Review            14:18  6400  472 149321
2024-10-06 Coke_Studio_India    Country_Mix          30:45  8700 1190 174543
2024-10-07 Fit_Tuber            Daily_Stretch        17:23  3200  243 100582
2024-10-06 Slayy_Point          Sandbox_Exploration  44:12  8000 1320 167530
2024-10-05 GeekyRanjit          Gadget_Tips          16:15  4900  278 115482
2024-10-04 BB_Ki_Vines          Classic_Rock         29:58  9000 1300 169384
2024-10-06 Flying_Beast         HIIT_Workout         22:30  5600  492 123842
2024-10-03 Mortal               RPG_Guide            18:58  6100  427 129903
2024-10-05 Emiway_Bantai        Festival_Highlights  39:45  7100  593 137528
2024-10-07 TechBurner           Game_Strategies      25:34  8000  523 143927
2024-10-06 Coke_Studio_India    Retro_Music          57:50  9300 1482 167283
2024-10-05 Technical_Guruji     Gadget_Review        14:25  4600  391 111847
2024-10-07 T-Series             Pop_Music            30:12  6700  749 136413
2024-10-05 Fit_Tuber            Cardio_Workout       15:45  3500  284 100275
2024-10-06 Mortal               Game_Analysis        19:50  5400  458 118942
2024-10-03 BB_Ki_Vines          Chill_Vibes          32:22  7100  602 135749
2024-10-04 Yoga_With_Adriene    Evening_Yoga         40:15  5200  431 114812
2024-10-05 Flying_Beast         Quick_Fitness        11:56  4500  279 113142
2024-10-03 Emiway_Bantai        Music_Review         26:13  5400  402 115932
2024-10-07 Technical_Guruji     Future_Technology    38:05  7800  565 128731
2024-10-06 Coke_Studio_India    Pop_Hits             23:15  8900 1472 159842
2024-10-05 Mortal               Battle_Tips          15:29  6700  521 140329
2024-10-06 BB_Ki_Vines          Live_Concert         57:38  9400 1294 168547
2024-10-05 TechBurner           Top_5_Games          20:03  7300  645 137023
2024-10-03 Flying_Beast         Cardio_Blast         17:54  6100  598 132581
2024-10-06 Emiway_Bantai        Acoustic_Cover       24:22  7600 1201 144233
2024-10-07 Fit_Tuber            Total_Body           31:19  5400  402 125187
2024-10-06 Coke_Studio_India    Instrumental         36:40  8100 1369 158475
2024-10-05 Yoga_With_Adriene    Flexibility          18:08  3900  294 102456
2024-10-07 TechBurner           Game_Updates         19:19  6500  411 136492
2024-10-03 BB_Ki_Vines          Relaxation           20:56  4700  267 118764
2024-10-06 Mortal               Strategy_Tips        25:03  7400 620  135782
2024-10-05 Fit_Tuber            Morning_Yoga         12:44  4500 356  100856
2024-10-07 Emiway_Bantai        Jam_Session          22:18  8300 1450 178194
2024-10-03 Technical_Guruji     Gadget_Hacks         16:55  5200 420  111843
2024-10-06 BB_Ki_Vines          Singer_Songwriter    43:35  9800 1200 158372
2024-10-05 Flying_Beast         Core_Training        13:13  3700 285  101294
2024-10-06 Mortal               Combat_Tactics       14:24  4900 398  117293
2024-10-07 Yoga_With_Adriene    Functional_Training  30:14  6600 572  128934
2024-10-05 Emiway_Bantai        Vocal_Performance    29:30  7800 764  133487
2024-10-06 BB_Ki_Vines          Electronic_Beats     40:45  9100 1354 163921
2024-10-07 Technical_Guruji     AI_Technology        18:15  6200 421  129301
2024-10-06 Coke_Studio_India    Classical_Music      57:49  8400 1392 174321

3)hadoop fs -put yt.txt yt.txt                                     //once type the same command to get file already exists as output
4)hadoop jar reals.jar reals.reals yt.txt dir11                   //Try giving different dir numbers for each programs 
5)hadoop fs -cat dir11/part-00000    	                         //Make sure u give the same dir number wt u gave in above command.	    	    
            </code>
        </div>
    </div>

        <!-- User Guide -->
    <div class="section">
        <div class="header" onclick="toggleContent('content6')">JAR FILES GUIDE</div>
        <div class="content" id="content6">
            <code>  
		    
AFTER CODE DO THIS:
Build Path-> Add External Archives-> File System-> Usr-> Lib-> hadoop.common.jar
Build Path-> Add External Archives-> File System-> Usr-> Lib-> hadoop-0.20-mapreduce-> hadoop-core-2.6.0-mr1-cdh5.13.0.jar
Build Path-> Add External Archives-> File System-> Usr-> Lib-> hadoop-> hadoop-auth.jar	

This will make to remove all the red lines in the copied code, if not, copy paste the code again and check the class name and package name properly.

Export the class File:
Export-> Jar File-> "Select the path:" /home/cloudera/"your_jar_file_name".jar   ----> Finish
		     
            </code>
        </div>
    </div>
</div> 	
</div>

<script>
function toggleContent(id) {
    var content = document.getElementById(id);
    if (content.style.display === "block") {
        content.style.display = "none";
    } else {
        content.style.display = "block";
    }
}
</script>

</body>
</html>
